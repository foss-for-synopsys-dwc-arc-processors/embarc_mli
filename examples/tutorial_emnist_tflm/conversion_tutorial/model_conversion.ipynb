{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting TensorFlow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLI Library v.1.1 introduces a new way of model deployment for target applications. MLI support has been integrated into Tensorflow Lite Micro framework which means that its tools are now available to MLI users as well. So, alternatively to manual graph mapping you can use TFLM converting capabilities to convert model to MLI compatible format.\n",
    "\n",
    "In this tutorial we'll cover Tensorflow model conversion specific for ARC target application. It implies that all model values (weights and activations) and preferably inputs and outputs should be converted to 8-bit integers.\n",
    "\n",
    "Since full integer quantization (the one that includes inputs and outputs) is supported in Tensorflow 2 staring from version 2.3 it is important that your setup provides this version or newer. Please, make sure that this and other requirements from *requirements.txt* are satisfied and let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "from tensorflow import keras\n",
    "    \n",
    "print(tf.__version__)\n",
    "assert tf.__version__ >= '2.3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a letter recognition model that was trained on EMNIST Letters dataset. Training of this model is described in details in [tutorial emnist tensorflow](https://github.com/foss-for-synopsys-dwc-arc-processors/embarc_mli/blob/mli_dev/examples/tutorial_emnist_tensorflow/example.ipynb) which covers manual mapping. In this tutorial we're just going to define model and load pretrained weights(*mli_cnn_bn.h5*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "num_classes = 26\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "filter_x = 5\n",
    "filter_y = 5\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Conv1\n",
    "model.add(Conv2D(filters=16, \n",
    "                 kernel_size=(filter_x, filter_y), \n",
    "                 padding=\"same\",  \n",
    "                 input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "#Conv2\n",
    "model.add(Conv2D(filters=32, \n",
    "                 kernel_size=(filter_x, filter_y), \n",
    "                 padding=\"same\", \n",
    "                 input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "#Conv3\n",
    "model.add(Conv2D(filters=32, \n",
    "                 kernel_size=(filter_x, filter_y), \n",
    "                 padding=\"same\", \n",
    "                 input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "#FC1\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "#FC2\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.load_weights('mli_cnn_bn.h5')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we're using pretrained model we still need images from EMNIST dataset for following conversion purposes:\n",
    "1. TFLiteConverter requires a representative dataset of input pictures to perform integer quantization\n",
    "2. Pictures will be used for converted model testing and evaluation\n",
    "\n",
    "Load letters test samples from EMNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from emnist import extract_test_samples\n",
    "\n",
    "test_images, test_labels = extract_test_samples('letters')\n",
    "\n",
    "# Make class numbering start at 0\n",
    "test_labels = test_labels - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model expects preprocessed images as inputs, so dataset images should undergo following preprocessing steps: \n",
    "1. Reshape (28,28) bitmaps as (28,28,1)\n",
    "2. Perform thinning: set values to either 0 or 255\n",
    "3. Normalize values, so the range becomes [-1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_test_images = test_images.reshape([test_images.shape[0], img_rows, img_cols, 1])\n",
    "\n",
    "def thinning(image):\n",
    "    tmp = np.where(image < 210.0, 0, image)\n",
    "    return np.where(tmp > 210.0, 255, tmp)\n",
    "\n",
    "preprocessed_test_images = thinning(preprocessed_test_images)\n",
    "\n",
    "preprocessed_test_images = (preprocessed_test_images - 128.0) / 128.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert model into TFLM format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model and EMNIST samples we can convert model using [TFLiteConverter](https://www.tensorflow.org/lite/convert/python_api). \n",
    "\n",
    "Setup TFLiteConverter to load the model and perform full integer quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a representative dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_test_images = tf.cast(preprocessed_test_images, tf.float32)\n",
    "emnist_ds = tf.data.Dataset.from_tensor_slices((preprocessed_test_images)).batch(1)\n",
    "\n",
    "def representative_data_gen():\n",
    "    for input_value in emnist_ds.take(100):\n",
    "        yield [input_value]\n",
    "    \n",
    "converter.representative_dataset = representative_data_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the model to TensorFlow Lite format and save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "converted_model = converter.convert()\n",
    "\n",
    "generated_dir = pathlib.Path(\"generated/\")\n",
    "generated_dir.mkdir(exist_ok=True, parents=True)\n",
    "converted_model_file = generated_dir/\"emnist_model_int8.tflite\"\n",
    "converted_model_file.write_bytes(converted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to integrate converted model into TFLM application we have to save it as a C array. One way to do that is to use **xxd** utility available on Linux or in Cygwin/MinGW terminals on Windows. Open terminal and run following commands:\n",
    "\n",
    "```\n",
    "cd generated/\n",
    "xxd -i emnist_model_int8.tflite > model.h\n",
    "```\n",
    "\n",
    "The model is ready to be integrated into TFLM application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that convertion went well let's run the model on a test dataset and check that accuracy is around 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=str(converted_model_file))\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to evaluate the TF Lite model using \"test\" dataset.\n",
    "def evaluate_model(interpreter):\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "    scale, zero_point = interpreter.get_output_details()[0]['quantization']\n",
    "\n",
    "    prediction_values = []\n",
    "    \n",
    "    for test_image in preprocessed_test_images:\n",
    "        # Pre-processing: add batch dimension, quantize and convert inputs to int8 to match with\n",
    "        # the model's input data format.\n",
    "        test_image = np.expand_dims(test_image, axis=0) #.astype(np.float32)\n",
    "        test_image = np.int8(test_image / scale + zero_point)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Find the letter with highest probability\n",
    "        output = interpreter.tensor(output_index)\n",
    "        result = np.argmax(output()[0])\n",
    "        prediction_values.append(result)\n",
    "    \n",
    "    accurate_count = 0\n",
    "    for index in range(len(prediction_values)):\n",
    "        if prediction_values[index] == test_labels[index]:\n",
    "            accurate_count += 1\n",
    "    accuracy = accurate_count * 1.0 / len(prediction_values)\n",
    "\n",
    "    return accuracy * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, keep in mind that full test dataset evaluation on int8 model may take several minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(evaluate_model(interpreter)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a test set for target application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test model in a target application we're going to generate a C file containing test samples gathered from EMNIST database. \n",
    "In our case samples are going to be randomly selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "num_of_samples = 25\n",
    "random_test_images = random.sample(range(1, test_images.shape[0]), num_of_samples)\n",
    "\n",
    "fig=plt.figure(figsize=(5, 5))\n",
    "cols = 5\n",
    "rows = 5\n",
    "\n",
    "for plt_idx, img_idx in enumerate(random_test_images, 1):\n",
    "    img = test_images[img_idx]\n",
    "    fig.add_subplot(rows, cols, plt_idx)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write samples to *test_samples.cc* file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "samples_file = open(\"generated/test_samples.cc\", \"w\")\n",
    "\n",
    "samples_file.write(\"#include \\\"test_samples.h\\\"\\n\\n\")\n",
    "samples_file.write(\"const int kNumSamples = \" + str(num_of_samples) + \";\\n\\n\")\n",
    "\n",
    "samples = \"\" \n",
    "samples_array = \"const TestSample test_samples[kNumSamples] = {\"\n",
    "\n",
    "for sample_idx, img_idx in enumerate(random_test_images, 1):\n",
    "    img_arr = list(np.ndarray.flatten(test_images[img_idx]))\n",
    "    var_name = \"sample\" + str(sample_idx)\n",
    "    samples += \"TestSample \" + var_name + \" = {\\n\" #+ \"[IMAGE_SIZE] = { \"\n",
    "    samples += \"\\t.label = \" + str(test_labels[img_idx]) + \",\\n\" \n",
    "    samples += \"\\t.image = {\\n\"\n",
    "    wrapped_arr = [img_arr[i:i + 20] for i in range(0, len(img_arr), 20)]\n",
    "    for sub_arr in wrapped_arr:\n",
    "        samples += \"\\t\\t\" + str(sub_arr)\n",
    "    samples += \"\\t}\\n};\\n\\n\"    \n",
    "    samples_array += var_name + \", \"\n",
    "    \n",
    "samples = samples.replace(\"[\", \"\")\n",
    "samples = samples.replace(\"]\", \",\\n\")\n",
    "samples_array += \"};\\n\"\n",
    "\n",
    "samples_file.write(samples);\n",
    "samples_file.write(samples_array);\n",
    "samples_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have converted a Tensorflow model into TFLM format and generated a test set for the application. Now you can copy generated files into target application of this tutorial and try it out:\n",
    "* copy *generated/model.h* and *generated/test_samples.cc* to *../src*\n",
    "* open top folder of this tutorial (*tutorial_emnist_tflm*) in terminal and build and run application:\n",
    "```\n",
    "make app run\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
