{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting TensorFlow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLI Library from version 1.1 introduces a new way of model deployment for target applications. MLI support has been integrated into Tensorflow Lite Micro framework which means that its tools are now available to MLI users as well. So, alternatively to manual graph mapping you can use TFLM converting capabilities to convert model to MLI compatible format. Also, from MLI Library version 2.0, some kernels uses new layout for weights and [Model Adaptation Tool](https://github.com/foss-for-synopsys-dwc-arc-processors/tflite-micro/tree/arc_mli_20_temp/tensorflow/lite/micro/tools/make/targets/arc#model-adaptation-tool-experimental-feature) is used to convert TFLM models to this new layout.\n",
    "\n",
    "In this tutorial we'll cover Tensorflow model conversion specific for ARC target application. It implies that all model values (weights and activations) and preferably inputs and outputs should be converted to 8-bit integers.\n",
    "\n",
    "Since full integer quantization (the one that includes inputs and outputs) is supported in Tensorflow 2 staring from version 2.3 it is important that your setup provides this version or newer. Please, make sure that this and other requirements from *requirements.txt* are satisfied and let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "from tensorflow import keras\n",
    "    \n",
    "print(tf.__version__)\n",
    "assert tf.__version__ >= '2.3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a letter recognition model that was trained on EMNIST Letters dataset. Training of this model is described in details in [tutorial emnist tensorflow](https://github.com/foss-for-synopsys-dwc-arc-processors/embarc_mli/blob/mli_dev/examples/tutorial_emnist_tensorflow/example.ipynb) which covers manual mapping. In this tutorial we're just going to define model and load pretrained weights(*mli_cnn_bn.h5*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "num_classes = 26\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "filter_x = 5\n",
    "filter_y = 5\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Conv1\n",
    "model.add(Conv2D(filters=16, \n",
    "                 kernel_size=(filter_x, filter_y), \n",
    "                 padding=\"same\",  \n",
    "                 input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "#Conv2\n",
    "model.add(Conv2D(filters=32, \n",
    "                 kernel_size=(filter_x, filter_y), \n",
    "                 padding=\"same\", \n",
    "                 input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "#Conv3\n",
    "model.add(Conv2D(filters=32, \n",
    "                 kernel_size=(filter_x, filter_y), \n",
    "                 padding=\"same\", \n",
    "                 input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "#FC1\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "#FC2\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.load_weights('mli_cnn_bn.h5')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we're using pretrained model we still need images from EMNIST dataset for following conversion purposes:\n",
    "1. TFLiteConverter requires a representative dataset of input pictures to perform integer quantization\n",
    "2. Pictures will be used for converted model testing and evaluation\n",
    "\n",
    "Load letters test samples from EMNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from emnist import extract_test_samples\n",
    "\n",
    "test_images, test_labels = extract_test_samples('letters')\n",
    "\n",
    "# Make class numbering start at 0\n",
    "test_labels = test_labels - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model expects preprocessed images as inputs, so dataset images should undergo following preprocessing steps: \n",
    "1. Reshape (28,28) bitmaps as (28,28,1)\n",
    "2. Perform thinning: set values to either 0 or 255\n",
    "3. Normalize values, so the range becomes [-1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_test_images = test_images.reshape([test_images.shape[0], img_rows, img_cols, 1])\n",
    "\n",
    "def thinning(image):\n",
    "    tmp = np.where(image < 210.0, 0, image)\n",
    "    return np.where(tmp > 210.0, 255, tmp)\n",
    "\n",
    "preprocessed_test_images = thinning(preprocessed_test_images)\n",
    "\n",
    "preprocessed_test_images = (preprocessed_test_images - 128.0) / 128.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert model into TFLM format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model and EMNIST samples we can convert model using [TFLiteConverter](https://www.tensorflow.org/lite/convert/python_api). \n",
    "\n",
    "Setup TFLiteConverter to load the model and perform full integer quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a representative dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_test_images = tf.cast(preprocessed_test_images, tf.float32)\n",
    "emnist_ds = tf.data.Dataset.from_tensor_slices((preprocessed_test_images)).batch(1)\n",
    "\n",
    "def representative_data_gen():\n",
    "    for input_value in emnist_ds.take(100):\n",
    "        yield [input_value]\n",
    "    \n",
    "converter.representative_dataset = representative_data_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the model to TensorFlow Lite format and save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "converted_model = converter.convert()\n",
    "\n",
    "generated_dir = pathlib.Path(\"generated/\")\n",
    "generated_dir.mkdir(exist_ok=True, parents=True)\n",
    "converted_model_file = generated_dir/\"emnist_model_int8.tflite\"\n",
    "converted_model_file.write_bytes(converted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that convertion went well let's run the model on a test dataset and check that accuracy is around 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=str(converted_model_file))\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full test set contains 20800 samples. Evaluating int8 model on it might take more than 10 minutes. If you want to get estimation faster, please, limit number of samples to be evaluated by reducing **max_samples** value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 20800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to evaluate the TF Lite model using \"test\" dataset.\n",
    "def evaluate_model(interpreter):\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "    scale, zero_point = interpreter.get_output_details()[0]['quantization']\n",
    "\n",
    "    prediction_values = []\n",
    "    \n",
    "    for test_image in preprocessed_test_images[:max_samples]:\n",
    "        # Pre-processing: add batch dimension, quantize and convert inputs to int8 to match with\n",
    "        # the model's input data format.\n",
    "        test_image = np.expand_dims(test_image, axis=0) #.astype(np.float32)\n",
    "        test_image = np.int8(test_image / scale + zero_point)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Find the letter with highest probability\n",
    "        output = interpreter.tensor(output_index)\n",
    "        result = np.argmax(output()[0])\n",
    "        prediction_values.append(result)\n",
    "    \n",
    "    accurate_count = 0\n",
    "    for index in range(len(prediction_values)):\n",
    "        if prediction_values[index] == test_labels[index]:\n",
    "            accurate_count += 1\n",
    "    accuracy = accurate_count * 1.0 / len(prediction_values)\n",
    "\n",
    "    return accuracy * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, keep in mind that full test dataset evaluation on int8 model may take several minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(evaluate_model(interpreter)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to integrate converted model into TFLM application we have to convert it to MLI 2.0 layout and then save it as a C array. \n",
    "Use [Model Adaptation Tool](https://github.com/foss-for-synopsys-dwc-arc-processors/tflite-micro/tree/arc_mli_20_temp/tensorflow/lite/micro/tools/make/targets/arc#model-adaptation-tool-experimental-feature) to convert `emnist_model_int8.tflite` you got to use it with embARC MLI Library 2.0:\n",
    "```\n",
    "python adaptation_tool.py <embarc_mli_directory>/examples/tutorial_emnist_tflm/conversion_tutorial/generated/emnist_model_int8.tflite\n",
    "```\n",
    "Save converted model as a C array. One way to do that is to use xxd utility available on Linux or in Cygwin/MinGW terminals on Windows. Open terminal and run following command:\n",
    "\n",
    "```\n",
    "cd generated/\n",
    "xxd -i emnist_model_int8.tflite > model.h\n",
    "```\n",
    "\n",
    "The model is ready to be integrated into TFLM application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a test set for target application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test model in a target application we're going to generate a C file containing test samples gathered from EMNIST database. \n",
    "In our case samples are going to be randomly selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "num_of_samples = 25\n",
    "random_test_images = random.sample(range(1, test_images.shape[0]), num_of_samples)\n",
    "\n",
    "fig=plt.figure(figsize=(5, 5))\n",
    "cols = 5\n",
    "rows = 5\n",
    "\n",
    "for plt_idx, img_idx in enumerate(random_test_images, 1):\n",
    "    img = test_images[img_idx]\n",
    "    fig.add_subplot(rows, cols, plt_idx)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write samples to *test_samples.cc* file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "samples_file = open(\"generated/test_samples.cc\", \"w\")\n",
    "\n",
    "samples_file.write(\"#include \\\"test_samples.h\\\"\\n\\n\")\n",
    "samples_file.write(\"const int kNumSamples = \" + str(num_of_samples) + \";\\n\\n\")\n",
    "\n",
    "samples = \"\" \n",
    "samples_array = \"const TestSample test_samples[kNumSamples] = {\"\n",
    "\n",
    "for sample_idx, img_idx in enumerate(random_test_images, 1):\n",
    "    img_arr = list(np.ndarray.flatten(test_images[img_idx]))\n",
    "    var_name = \"sample\" + str(sample_idx)\n",
    "    samples += \"TestSample \" + var_name + \" = {\\n\" #+ \"[IMAGE_SIZE] = { \"\n",
    "    samples += \"\\t.label = \" + str(test_labels[img_idx]) + \",\\n\" \n",
    "    samples += \"\\t.image = {\\n\"\n",
    "    wrapped_arr = [img_arr[i:i + 20] for i in range(0, len(img_arr), 20)]\n",
    "    for sub_arr in wrapped_arr:\n",
    "        samples += \"\\t\\t\" + str(sub_arr)\n",
    "    samples += \"\\t}\\n};\\n\\n\"    \n",
    "    samples_array += var_name + \", \"\n",
    "    \n",
    "samples = samples.replace(\"[\", \"\")\n",
    "samples = samples.replace(\"]\", \",\\n\")\n",
    "samples_array += \"};\\n\"\n",
    "\n",
    "samples_file.write(samples);\n",
    "samples_file.write(samples_array);\n",
    "samples_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have converted a Tensorflow model into TFLM format and generated a test set for the application. Now you can copy generated files into target application of this tutorial and try it out:\n",
    "* copy *generated/model.h* and *generated/test_samples.cc* to *../src*\n",
    "* open top folder of this tutorial (*tutorial_emnist_tflm*) in terminal and build and run application\n",
    "\n",
    "You can build the example using MetaWare Development Tools:\n",
    "```\n",
    "gmake app TCF_FILE=<path_to_vpx_tcf_file>\n",
    "```\n",
    "And run the example with DesignWare ARC nSIM simulator:\n",
    "```\n",
    "gmake run TCF_FILE=<path_to_vpx_tcf_file>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
